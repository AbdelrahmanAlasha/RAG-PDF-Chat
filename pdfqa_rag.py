# -*- coding: utf-8 -*-
"""Simple_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHO4vFg6sr40lmU6z1NDxeG_34T6UyNM
"""

!pip install -U sentence-transformers PyPDF2 numpy requests

import PyPDF2

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file using PyPDF2.
    Args:
        pdf_path (str): Path to the PDF file.
    Returns:
        str: Extracted text from the PDF.
    """
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

pdf_text = extract_text_from_pdf("DetectGPT_Zero-Shot_Machine-Generated_Text_Detecti.pdf")

from sentence_transformers import SentenceTransformer
import numpy as np

# Initialize the model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Split text into chunks
def split_text(text, chunk_size=500):
    """
    Splits text into smaller chunks of specified size.
    Args:
        text (str): The text to split.
        chunk_size (int): Size of each chunk.
    Returns:
        list: A list of text chunks.
    """
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

chunks = split_text(pdf_text)

# Generate embeddings for each chunk
document_embeddings = model.encode(chunks, show_progress_bar=True)

from sklearn.metrics.pairwise import cosine_similarity

def retrieve_with_cosine_similarity(query, model, document_embeddings, chunks, top_k=3):
    """
    Retrieves the top-k most relevant chunks for a query using cosine similarity.
    Args:
        query (str): User query.
        model (SentenceTransformer): Embedding model.
        document_embeddings (numpy.ndarray): Array of document embeddings.
        chunks (list): List of document chunks.
        top_k (int): Number of results to return.
    Returns:
        list: Top-k most relevant chunks.
    """
    # Encode the query to get its embedding
    query_embedding = model.encode([query])

    # Compute cosine similarity
    similarities = cosine_similarity(query_embedding, document_embeddings)[0]

    # Get the indices of the top-k most similar documents
    top_indices = similarities.argsort()[-top_k:][::-1]

    # Return the top-k most relevant chunks
    return [chunks[i] for i in top_indices]

import requests

def generate_response(prompt, api_key):
    """
    Sends a request to the Gemini AI API to generate a response.
    Args:
        text (str): Input text.
        api_key (str): Your Gemini AI API key.
    Returns:
        str: Generated response.
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    response = requests.post(url, headers=headers, json=payload)
    return response.json()

api_key = "API_KEY"

while True:
    query = input("You: ")
    if query.lower() in ["exit", "quit"]:
        break

    retrieved_chunks = retrieve_with_cosine_similarity(query, model, document_embeddings, chunks)
    context = "\n".join(retrieved_chunks)

    prompt = f"""Context information is below.
---------------------
{context}
---------------------
Given the context information above I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.
Query: {query}
Answer: """

    response = generate_response(prompt, api_key)
    print(f"Bot: {response['candidates'][0]['content']['parts'][0]['text']}")

